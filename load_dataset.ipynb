{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import jsonlines\n",
    "\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1260\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_dataset = load_dataset(\"lamini/lamini_docs\", split=\"train\")\n",
    "finetuned_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuned_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     pprint(finetuned_dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revise map, filter and reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vee_list = [3, 4, 5]\n",
    "\n",
    "add_list = []\n",
    "\n",
    "for item in vee_list:\n",
    "    if item > 4:\n",
    "        add_list.append(item)\n",
    "    \n",
    "add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(map(lambda x: x + 1, vee_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x > 4, vee_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "reduce(lambda x, y: x * y, vee_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: Create `lamini_docs.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': 'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?'}\n"
     ]
    }
   ],
   "source": [
    "new_list = list(\n",
    "    map(\n",
    "        lambda lamini_dict: {\n",
    "            \"question\": lamini_dict[\"question\"],\n",
    "            \"answer\": lamini_dict[\"answer\"],\n",
    "        },\n",
    "        finetuned_dataset,\n",
    "    )\n",
    ")\n",
    "\n",
    "assert len(new_list) == len(finetuned_dataset)\n",
    "\n",
    "pprint(new_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f\"json/lamini_docs.jsonl\", \"w\") as writer:\n",
    "    writer.write_all(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare pre-training and fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained dataset:\n",
      "{'text': 'Beginners BBQ Class Taking Place in Missoula!\\n'\n",
      "         'Do you want to get better at making delicious BBQ? You will have the '\n",
      "         'opportunity, put this on your calendar now. Thursday, September 22nd '\n",
      "         'join World Class BBQ Champion, Tony Balay from Lonestar Smoke '\n",
      "         'Rangers. He will be teaching a beginner level class for everyone who '\n",
      "         'wants to get better with their culinary skills.\\n'\n",
      "         'He will teach you everything you need to know to compete in a KCBS '\n",
      "         'BBQ competition, including techniques, recipes, timelines, meat '\n",
      "         'selection and trimming, plus smoker and fire information.\\n'\n",
      "         'The cost to be in the class is $35 per person, and for spectators it '\n",
      "         'is free. Included in the cost will be either a t-shirt or apron and '\n",
      "         'you will be tasting samples of each meat that is prepared.',\n",
      " 'timestamp': '2019-04-25T12:57:54Z',\n",
      " 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\n",
      "{'text': \"Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, \"\n",
      "         '2012.\\n'\n",
      "         \"I've got a 500gb internal drive and a 240gb SSD.\\n\"\n",
      "         'When trying to restore using disk utility i\\'m given the error \"Not '\n",
      "         'enough space on disk ____ to restore\"\\n'\n",
      "         \"But I shouldn't have to do that!!!\\n\"\n",
      "         'Any ideas or workarounds before resorting to the above?\\n'\n",
      "         \"Use Carbon Copy Cloner to copy one drive to the other. I've done \"\n",
      "         'this several times going from larger HDD to smaller SSD and I wound '\n",
      "         'up with a bootable SSD drive. One step you have to remember not to '\n",
      "         'skip is to use Disk Utility to partition the SSD as GUID partition '\n",
      "         'scheme HFS+ before doing the clone. If it came Apple Partition '\n",
      "         \"Scheme, even if you let CCC do the clone, the resulting drive won't \"\n",
      "         'be bootable. CCC usually works in \"file mode\" and it can easily copy '\n",
      "         \"a larger drive (that's mostly empty) onto a smaller drive. If you \"\n",
      "         'tell CCC to clone a drive you did NOT boot from, it can work in '\n",
      "         'block copy mode where the destination drive must be the same size or '\n",
      "         'larger than the drive you are cloning from (if I recall).\\n'\n",
      "         \"I've actually done this somehow on Disk Utility several times \"\n",
      "         '(booting from a different drive (or even the dvd) so not running '\n",
      "         'disk utility from the drive your cloning) and had it work just fine '\n",
      "         'from larger to smaller bootable clone. Definitely format the drive '\n",
      "         'cloning to first, as bootable Apple etc..\\n'\n",
      "         'Thanks for pointing this out. My only experience using DU to go '\n",
      "         'larger to smaller was when I was trying to make a Lion install stick '\n",
      "         'and I was unable to restore InstallESD.dmg to a 4 GB USB stick but '\n",
      "         \"of course the reason that wouldn't fit is there was slightly more \"\n",
      "         'than 4 GB of data.',\n",
      " 'timestamp': '2019-04-21T10:07:13Z',\n",
      " 'url': 'https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/'}\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "print(\"Pretrained dataset:\")\n",
    "top_n = itertools.islice(pretrained_dataset, n)\n",
    "for i in top_n:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I evaluate the performance and quality...</td>\n",
       "      <td>There are several metrics that can be used to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I find information about the code's approa...</td>\n",
       "      <td>Yes, the code includes methods for submitting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does Lamini AI handle requests for generat...</td>\n",
       "      <td>Lamini AI offers features for generating text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the `submit_job()` function expose any ad...</td>\n",
       "      <td>It is unclear which `submit_job()` function is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the `add_data()` function support differe...</td>\n",
       "      <td>No, the `add_data()` function does not support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>Does the documentation provide guidelines for ...</td>\n",
       "      <td>There is no mention of memory caching or evict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Does Lamini provide any mechanisms for model e...</td>\n",
       "      <td>Yes, Lamini provides mechanisms for model ense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>Is Lamini owned by Tesla?</td>\n",
       "      <td>No, Lamini AI is an independent company workin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>What is the process for suggesting edits or im...</td>\n",
       "      <td>You can suggest edits or improvements to the L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>How frequently should we call the `check_job_s...</td>\n",
       "      <td>The frequency of calling the `check_job_status...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     How can I evaluate the performance and quality...   \n",
       "1     Can I find information about the code's approa...   \n",
       "2     How does Lamini AI handle requests for generat...   \n",
       "3     Does the `submit_job()` function expose any ad...   \n",
       "4     Does the `add_data()` function support differe...   \n",
       "...                                                 ...   \n",
       "1255  Does the documentation provide guidelines for ...   \n",
       "1256  Does Lamini provide any mechanisms for model e...   \n",
       "1257                          Is Lamini owned by Tesla?   \n",
       "1258  What is the process for suggesting edits or im...   \n",
       "1259  How frequently should we call the `check_job_s...   \n",
       "\n",
       "                                                 answer  \n",
       "0     There are several metrics that can be used to ...  \n",
       "1     Yes, the code includes methods for submitting ...  \n",
       "2     Lamini AI offers features for generating text ...  \n",
       "3     It is unclear which `submit_job()` function is...  \n",
       "4     No, the `add_data()` function does not support...  \n",
       "...                                                 ...  \n",
       "1255  There is no mention of memory caching or evict...  \n",
       "1256  Yes, Lamini provides mechanisms for model ense...  \n",
       "1257  No, Lamini AI is an independent company workin...  \n",
       "1258  You can suggest edits or improvements to the L...  \n",
       "1259  The frequency of calling the `check_job_status...  \n",
       "\n",
       "[1260 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"json/lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "instruction_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How can I evaluate the performance and quality of the generated text from '\n",
      " 'Lamini models?There are several metrics that can be used to evaluate the '\n",
      " 'performance and quality of generated text from Lamini models, including '\n",
      " 'perplexity, BLEU score, and human evaluation. Perplexity measures how well '\n",
      " 'the model predicts the next word in a sequence, while BLEU score measures '\n",
      " 'the similarity between the generated text and a reference text. Human '\n",
      " 'evaluation involves having human judges rate the quality of the generated '\n",
      " 'text based on factors such as coherence, fluency, and relevance. It is '\n",
      " 'recommended to use a combination of these metrics for a comprehensive '\n",
      " \"evaluation of the model's performance.\")\n"
     ]
    }
   ],
   "source": [
    "examples = instruction_dataset_df.to_dict()\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vee is very fond of visiting Calabar'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"{name} is very fond of visiting {place}\"\n",
    "\n",
    "word.format(name=\"Vee\", place=\"Calabar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vee is very fond of visiting Calabar'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, place = \"Vee\", \"Calabar\"\n",
    "\n",
    "f\"{name} is very fond of visiting {place}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_qa = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('### Question:\\n'\n",
      " 'How can I evaluate the performance and quality of the generated text from '\n",
      " 'Lamini models?\\n'\n",
      " '\\n'\n",
      " '### Answer:\\n'\n",
      " 'There are several metrics that can be used to evaluate the performance and '\n",
      " 'quality of generated text from Lamini models, including perplexity, BLEU '\n",
      " 'score, and human evaluation. Perplexity measures how well the model predicts '\n",
      " 'the next word in a sequence, while BLEU score measures the similarity '\n",
      " 'between the generated text and a reference text. Human evaluation involves '\n",
      " 'having human judges rate the quality of the generated text based on factors '\n",
      " 'such as coherence, fluency, and relevance. It is recommended to use a '\n",
      " \"combination of these metrics for a comprehensive evaluation of the model's \"\n",
      " 'performance.')\n"
     ]
    }
   ],
   "source": [
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
    "pprint(text_with_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_q = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only = []\n",
    "finetuning_dataset_question_answer = []\n",
    "\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "\n",
    "    text_with_prompt_template_qa = prompt_template_qa.format(\n",
    "        question=question, answer=answer\n",
    "    )\n",
    "    finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "    text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "    finetuning_dataset_question_answer.append(\n",
    "        {\"question\": text_with_prompt_template_q, \"answer\": answer}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Question:\\n'\n",
      "         'How can I evaluate the performance and quality of the generated text '\n",
      "         'from Lamini models?\\n'\n",
      "         '\\n'\n",
      "         '### Answer:\\n'\n",
      "         'There are several metrics that can be used to evaluate the '\n",
      "         'performance and quality of generated text from Lamini models, '\n",
      "         'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "         'measures how well the model predicts the next word in a sequence, '\n",
      "         'while BLEU score measures the similarity between the generated text '\n",
      "         'and a reference text. Human evaluation involves having human judges '\n",
      "         'rate the quality of the generated text based on factors such as '\n",
      "         'coherence, fluency, and relevance. It is recommended to use a '\n",
      "         'combination of these metrics for a comprehensive evaluation of the '\n",
      "         \"model's performance.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_text_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f\"json/lamini_docs_processed.jsonl\", \"w\") as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
